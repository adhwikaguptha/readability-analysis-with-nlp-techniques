{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPy0ao6U42639fNaIlvtQse",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adhwikaguptha/readability-analysis-with-nlp-techniques/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRXqWB3GA7VP"
      },
      "outputs": [],
      "source": [
        "!pip install pandas openpyxl requests beautifulsoup4 nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "\n",
        "\n",
        "# Load the Excel file\n",
        "input_file = \"/content/sample_data/Input.xlsx\"  # Change this to your actual file path\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Display the first few rows to check the structure\n",
        "df.head()\n",
        "\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Display the first few rows to check the structure\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "owTiED59BYTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # Raise an error for bad responses (4xx, 5xx)\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "\n",
        "        for script in soup([\"script\", \"style\"]):  # Remove scripts and styles\n",
        "            script.decompose()\n",
        "\n",
        "        text = \" \".join(soup.stripped_strings)  # Join text while stripping extra spaces\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract text from {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "df['Extracted Text'] = df['URL'].apply(extract_text_from_url)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "q_5nk9CkB_tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in the above code\n",
        "BeautifulSoup extracts HTML elements from the webpage.\n",
        "soup.stripped_strings extracts all visible text elements.\n",
        "\" \".join(...) ensures proper spacing between words.\n"
      ],
      "metadata": {
        "id": "hmTWXibMWn-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "05FtHBmKXsNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W+', ' ', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "\n",
        "df['Cleaned Text'] = df['Extracted Text'].apply(clean_text)\n",
        "\n",
        "# Display results\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "QPyKaxO6C-UA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code Iam cleaning the code by converting it into lower case .Cleaning it by removing stop words and special characters nltk. Ensures that the stopword dataset is available.\n",
        "Retrieves a list of common English\n",
        "Converts it into a set for faster lookup."
      ],
      "metadata": {
        "id": "oispi1-QY24p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_excel(\"preprocessed_output.xlsx\", index=False)\n",
        "print(\"Preprocessed data saved to preprocessed_output.xlsx\")\n"
      ],
      "metadata": {
        "id": "D_RqOoMxDFFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"preprocessed_output.xlsx\")\n"
      ],
      "metadata": {
        "id": "YsOfc6nEDUoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib.util\n",
        "\n",
        "def is_installed(package_name):\n",
        "    return importlib.util.find_spec(package_name) is not None\n",
        "import importlib.util\n",
        "\n",
        "print(\"textstat installed:\", is_installed(\"textstat\"))"
      ],
      "metadata": {
        "id": "RAHUSzFVFa5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def install_if_missing(package_name):\n",
        "    if not is_installed(package_name):\n",
        "        print(f\"Installing {package_name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "\n",
        "install_if_missing(\"pandas\")\n",
        "install_if_missing(\"openpyxl\")\n",
        "install_if_missing(\"nltk\")\n",
        "install_if_missing(\"textstat\")\n",
        "\n",
        "print(\"All required packages are installed!\")\n"
      ],
      "metadata": {
        "id": "LOKcYM_CFsxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textstat import textstat\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "AfR17686F0zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_word_list(filepath):\n",
        "    \"\"\" Load words from a file into a set \"\"\"\n",
        "    with open(filepath, \"r\", encoding=\"latin-1\") as file:\n",
        "        return {line.strip().lower() for line in file if line.strip()}\n",
        "\n",
        "positive_words = load_word_list(\"/content/sample_data/positive-words.txt\")\n",
        "negative_words = load_word_list(\"/content/sample_data/negative-words.txt\")\n",
        "def get_positive_score(text):\n",
        "    words = text.split()\n",
        "    return sum(1 for word in words if word.lower() in positive_words)\n",
        "\n",
        "def get_negative_score(text):\n",
        "    words = text.split()\n",
        "    return sum(1 for word in words if word.lower() in negative_words) * -1  # Convert to positive value\n",
        "\n",
        "def get_polarity_score(pos_score, neg_score):\n",
        "    return (pos_score - neg_score) / ((pos_score + neg_score) + 0.000001)\n",
        "\n",
        "def get_subjectivity_score(pos_score, neg_score, total_words):\n",
        "    return (pos_score + neg_score) / (total_words + 0.000001)\n",
        "def get_avg_sentence_length(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(text.split())\n",
        "    return num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "def get_complex_word_count(text):\n",
        "    words = text.split()\n",
        "    return sum(1 for word in words if syllable_count(word) > 2)\n",
        "\n",
        "def get_percentage_complex_words(complex_word_count, total_words):\n",
        "    return (complex_word_count / total_words) * 100 if total_words > 0 else 0\n",
        "\n",
        "def get_fog_index(avg_sentence_length, percentage_complex_words):\n",
        "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "def get_avg_words_per_sentence(text):\n",
        "    return get_avg_sentence_length(text)\n",
        "\n",
        "def get_word_count(text):\n",
        "    words = text.split()\n",
        "    return len([word for word in words if word not in stop_words])\n",
        "\n",
        "def syllable_count(word):\n",
        "    \"\"\" Count syllables in a word \"\"\"\n",
        "    word = word.lower()\n",
        "    count = len(re.findall(r'[aeiouy]+', word))\n",
        "    count -= word.endswith((\"es\", \"ed\"))\n",
        "    return max(1, count)\n",
        "\n",
        "def get_syllables_per_word(text):\n",
        "    words = text.split()\n",
        "    total_syllables = sum(syllable_count(word) for word in words)\n",
        "    return total_syllables / len(words) if len(words) > 0 else 0\n",
        "\n",
        "def get_personal_pronouns(text):\n",
        "    \"\"\" Count occurrences of personal pronouns (excluding 'US') \"\"\"\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE)\n",
        "    return len(pronouns)\n",
        "\n",
        "def get_avg_word_length(text):\n",
        "    words = text.split()\n",
        "    total_chars = sum(len(word) for word in words)\n",
        "    return total_chars / len(words) if len(words) > 0 else 0\n",
        "df[\"Positive Score\"] = df[\"Cleaned Text\"].apply(get_positive_score)\n",
        "df[\"Negative Score\"] = df[\"Cleaned Text\"].apply(get_negative_score)\n",
        "df[\"Polarity Score\"] = df.apply(lambda x: get_polarity_score(x[\"Positive Score\"], x[\"Negative Score\"]), axis=1)\n",
        "df[\"Subjectivity Score\"] = df.apply(lambda x: get_subjectivity_score(x[\"Positive Score\"], x[\"Negative Score\"], len(x[\"Cleaned Text\"].split())), axis=1)\n",
        "\n",
        "df[\"Average Sentence Length\"] = df[\"Cleaned Text\"].apply(get_avg_sentence_length)\n",
        "df[\"Complex Word Count\"] = df[\"Cleaned Text\"].apply(get_complex_word_count)\n",
        "df[\"Percentage of Complex Words\"] = df.apply(lambda x: get_percentage_complex_words(x[\"Complex Word Count\"], len(x[\"Cleaned Text\"].split())), axis=1)\n",
        "df[\"Fog Index\"] = df.apply(lambda x: get_fog_index(x[\"Average Sentence Length\"], x[\"Percentage of Complex Words\"]), axis=1)\n",
        "df[\"Average Number of Words Per Sentence\"] = df[\"Cleaned Text\"].apply(get_avg_words_per_sentence)\n",
        "df[\"Word Count\"] = df[\"Cleaned Text\"].apply(get_word_count)\n",
        "df[\"Syllables Per Word\"] = df[\"Cleaned Text\"].apply(get_syllables_per_word)\n",
        "df[\"Personal Pronouns\"] = df[\"Cleaned Text\"].apply(get_personal_pronouns)\n",
        "df[\"Average Word Length\"] = df[\"Cleaned Text\"].apply(get_avg_word_length)\n",
        "\n",
        "# Saving the results\n",
        "df.to_excel(\"sentiment_analysis_output.xlsx\", index=False)\n",
        "print(\"Sentiment analysis completed! Results saved to sentiment_analysis_output.xlsx\")\n"
      ],
      "metadata": {
        "id": "hhXKOrUUGcpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "MZr-q20jGroS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "AmK-i5xpHx6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable_count(word):\n",
        "    \"\"\" Count syllables in a word \"\"\"\n",
        "    word = word.lower()\n",
        "    count = len(re.findall(r'[aeiouy]+', word))\n",
        "    count -= word.endswith((\"es\", \"ed\"))\n",
        "    return max(1, count)\n",
        "\n",
        "def get_syllables_per_word(text):\n",
        "    words = text.split()\n",
        "    total_syllables = sum(syllable_count(word) for word in words)\n",
        "    return total_syllables / len(words) if len(words) > 0 else 0\n",
        "def get_personal_pronouns(text):\n",
        "    \"\"\" Count occurrences of personal pronouns (excluding 'US') \"\"\"\n",
        "    pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE)\n",
        "    return len(pronouns)\n",
        "def get_avg_word_length(text):\n",
        "    words = text.split()\n",
        "    total_chars = sum(len(word) for word in words)\n",
        "    return total_chars / len(words) if len(words) > 0 else 0\n",
        "def get_complex_word_count(text):\n",
        "    words = text.split()\n",
        "    return sum(1 for word in words if syllable_count(word) > 2)\n",
        "\n",
        "def get_percentage_complex_words(complex_word_count, total_words):\n",
        "    return (complex_word_count / total_words) * 100 if total_words > 0 else 0\n",
        "def get_avg_sentence_length(text):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(text.split())\n",
        "    return num_words / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "def get_fog_index(avg_sentence_length, percentage_complex_words):\n",
        "    return 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "\n",
        "df = pd.read_excel(\"preprocessed_output.xlsx\")\n",
        "\n",
        "df[\"Syllables Per Word\"] = df[\"Cleaned Text\"].apply(get_syllables_per_word)\n",
        "df[\"Personal Pronouns\"] = df[\"Cleaned Text\"].apply(get_personal_pronouns)\n",
        "df[\"Average Word Length\"] = df[\"Cleaned Text\"].apply(get_avg_word_length)\n",
        "\n",
        "df[\"Complex Word Count\"] = df[\"Cleaned Text\"].apply(get_complex_word_count)\n",
        "df[\"Percentage of Complex Words\"] = df.apply(lambda x: get_percentage_complex_words(x[\"Complex Word Count\"], len(x[\"Cleaned Text\"].split())), axis=1)\n",
        "\n",
        "df[\"Average Sentence Length\"] = df[\"Cleaned Text\"].apply(get_avg_sentence_length)\n",
        "df[\"Fog Index\"] = df.apply(lambda x: get_fog_index(x[\"Average Sentence Length\"], x[\"Percentage of Complex Words\"]), axis=1)\n",
        "\n",
        "\n",
        "df.to_excel(\"advanced_text_analysis.xlsx\", index=False)\n",
        "print(\"Advanced text analysis completed! Results saved to advanced_text_analysis.xlsx\")\n"
      ],
      "metadata": {
        "id": "joE617qCH1uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"sentiment_analysis_output.xlsx\")\n"
      ],
      "metadata": {
        "id": "ugP1jtPDIMHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "generated_df = pd.read_excel(\"advanced_text_analysis.xlsx\")  # Your computed results\n",
        "expected_df = pd.read_excel(\"/content/sample_data/Output Data Structure.xlsx\")  # Expected results provided by the company\n",
        "\n",
        "# Display first few rows\n",
        "print(\"Generated Data Preview:\")\n",
        "display(generated_df.head())\n",
        "\n",
        "print(\"\\nExpected Data Preview:\")\n",
        "display(expected_df.head())\n"
      ],
      "metadata": {
        "id": "pzuuwIi-JkDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing text values with an empty string\n",
        "generated_df[\"Cleaned Text\"].fillna(\"\", inplace=True)\n"
      ],
      "metadata": {
        "id": "U8YPIjkrKIJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If URL is empty, flag it\n",
        "generated_df[\"Missing URL\"] = generated_df[\"URL\"].isna()\n"
      ],
      "metadata": {
        "id": "0mh5NODFKRYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure text encoding consistency\n",
        "generated_df[\"Cleaned Text\"] = generated_df[\"Cleaned Text\"].astype(str).apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8'))\n"
      ],
      "metadata": {
        "id": "mJAER50hKV_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the verified and cleaned results\n",
        "generated_df.to_excel(\"final_output.xlsx\", index=False)\n",
        "print(\"Final output saved as final_output.xlsx\")\n"
      ],
      "metadata": {
        "id": "9IOfW_3iKajs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"final_output.xlsx\")"
      ],
      "metadata": {
        "id": "PqYzICWlKmM7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}